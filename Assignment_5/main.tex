\documentclass[journal,12pt,twocolumn]{IEEEtran}
%
\usepackage{setspace}
\usepackage{gensymb}
\usepackage{siunitx}
\usepackage{tkz-euclide} 
\usepackage{textcomp}
\usepackage{standalone}
\usetikzlibrary{calc}
\newcommand\hmmax{0}
\newcommand\bmmax{0}

%\doublespacing
\singlespacing

%\usepackage{graphicx}
%\usepackage{amssymb}
%\usepackage{relsize}
\usepackage[cmex10]{amsmath}
%\usepackage{amsthm}
%\interdisplaylinepenalty=2500
%\savesymbol{iint}
%\usepackage{txfonts}
%\restoresymbol{TXF}{iint}
%\usepackage{wasysym}
\usepackage{amsthm}
%\usepackage{iithtlc}
\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{bm}
\usepackage{cite}
\usepackage{cases}
\usepackage{subfig}
%\usepackage{xtab}
\usepackage{longtable}
\usepackage{multirow}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{steinmetz}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{verbatim}
\usepackage{tfrupee}
\usepackage[breaklinks=true]{hyperref}
%\usepackage{stmaryrd}
\usepackage{tkz-euclide} % loads  TikZ and tkz-base
%\usetkzobj{all}
\usetikzlibrary{calc,math}
\usepackage{listings}
    \usepackage{color}                                            %%
    \usepackage{array}                                            %%
    \usepackage{longtable}                                        %%
    \usepackage{calc}                                             %%
    \usepackage{multirow}                                         %%
    \usepackage{hhline}                                           %%
    \usepackage{ifthen}                                           %%
  %optionally (for landscape tables embedded in another document): %%
    \usepackage{lscape}     
\usepackage{multicol}
\usepackage{chngcntr}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{amsmath}
%\usepackage{enumerate}

%\usepackage{wasysym}
%\newcounter{MYtempeqncnt}
\DeclareMathOperator*{\Res}{Res}
%\renewcommand{\baselinestretch}{2}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\def\inputGnumericTable{}                                 %%

\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible
}
%\lstset{
%language=tex,
%frame=single, 
%breaklines=true
%}
\usepackage{graphicx}
\usepackage{pgfplots}

\begin{document}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}
%\newtheorem{thm}{Theorem}[section] 
%\newtheorem{defn}[thm]{Definition}
%\newtheorem{algorithm}{Algorithm}[section]
%\newtheorem{cor}{Corollary}
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\bibliographystyle{IEEEtran}
%\bibliographystyle{ieeetr}
\providecommand{\mbf}{\mathbf}
\providecommand{\abs}[1]{\ensuremath{\left\vert#1\right\vert}}
\providecommand{\norm}[1]{\ensuremath{\left\lVert#1\right\rVert}}
\providecommand{\mean}[1]{\ensuremath{E\left[ #1 \right]}}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
%\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
%\providecommand{\hilbert}{\overset{\mathcal{H}}{ \rightleftharpoons}}
\providecommand{\system}{\overset{\mathcal{H}}{ \longleftrightarrow}}
	%\newcommand{\solution}[2]{\textbf{Solution:}{#1}}
\newcommand{\solution}{\noindent \textbf{Solution: }}
\newcommand{\cosec}{\,\text{cosec}\,}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
\newcommand{\mydet}[1]{\ensuremath{\begin{vmatrix}#1\end{vmatrix}}}
%\numberwithin{equation}{section}
\numberwithin{equation}{subsection}
%\numberwithin{problem}{section}
%\numberwithin{definition}{section}
\makeatletter
\@addtoreset{figure}{problem}
\makeatother
\let\StandardTheFigure\thefigure
\let\vec\mathbf
%\renewcommand{\thefigure}{\theproblem.\arabic{figure}}
\renewcommand{\thefigure}{\theproblem}
%\setlist[enumerate,1]{before=\renewcommand\theequation{\theenumi.\arabic{equation}}
%\counterwithin{equation}{enumi}
%\renewcommand{\theequation}{\arabic{subsection}.\arabic{equation}}
\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
\vspace{3cm}
\title{Polynomial Curve Fitting}
\maketitle
\newpage
%\tableofcontents
\bigskip
\renewcommand{\thefigure}{\theenumi}
\renewcommand{\thetable}{\theenumi}
\begin{abstract}
This document contains theory involved in curve fitting.
\end{abstract}
\section{\textbf{Objective}}
The objective is to fit best line for the polynomial curve using regularization.
\section{Generate Dataset}
Create a sinusoidal function of the form
\begin{align}
    y = A\sin{2\pi x} + n(t) \label{eq:1}
\end{align}
n(t) is the random noise that is included in the training set. This set consists of N samples of input data i.e. x expressed as shown below
\begin{align}
    x = \myvec{x_{1}, x_{2}, .., x_{N}}^{T}
\end{align}
which give the corresponding values of y denoted as
\begin{align}
    y = \myvec{y_{1}, y_{2}, .., y_{N}}^{T}
\end{align}
\begin{figure}[!h]
\begin{center}
\includegraphics[width=3.4in]{figs/fig1.png}
\end{center}
\caption{Sinusoidal Dataset with added noise}
\label{fig:1}
\end{figure}
The Fig \ref{fig:1} is generated by random values of $x_{n}$ for n =1,2,..,N.
where N=50 in the range [0,1].

The corresponding values of y were generated from the Eq \eqref{eq:1}.The first term $A\sin{2\pi x}$ is computed directly and then random noise samples having a normal(Gaussian) distribution are added inorder to get the corresponding values of y.
\begin{lstlisting}
#Generate the sine curve 
import numpy as np
import matplotlib.pyplot as plt

N = 50
np.random.seed(20)
x = np.sort(np.random.rand(N,1),axis=0)
noise = np.random.normal(0,0.3,size=(N,1))
A = 2.5
y = A*np.sin(2*np.pi*x) + noise

plt.scatter(x,y,c='b',marker='o',label='Data with noise')
plt.xlabel('x');plt.ylabel('y')
\end{lstlisting}

The following code generates the input matrix F
\begin{lstlisting}
sk_poly_deg=3
poly_feature = PolynomialFeatures(degree=sk_poly_deg,include_bias=False)
F = poly_feature.fit_transform(x)
\end{lstlisting}

The generated matrix would look like
\begin{align}
    \vec{F}= \myvec{ 1 & x_{0} & x_{0}^2 & \ldots & x_{0}^{N-1} \\
		1 & x_{1} & x_{1}^2 & \ldots & x_{1}^{N-1} \\
		1 & x_{2} & x_{2}^2 & \ldots & x_{2}^{N-1} \\
		\vdots & & \vdots &  & \vdots  \\
		    1 & \ldots & \ldots & \ldots & x_{N}^{N-1} }\label{eq:12}
\end{align}
\section{Polynomial Curve Fitting}
The goal is to find the best line that fits into the  pattern of the training data shown in the graph.
We shall fit the data using a polynomial function of the form, 
\begin{align}
     y\brak{w,x}= \sum_{j=0}^{M} w_j x^{j}\\
\end{align}
M is the order of the polynomial
The polynomial coefficient are collectively denoted by the vector $\vec{w}$.The proposed vector $\vec{w}$ of the model referring to Eq \eqref{eq:12} is given by 
\begin{align}
    \hat{\vec{w}} = \brak{\vec{F}^T\vec{F}}^{-1}\vec{F}^Ty \label{eq:13}
\end{align}

\section{Bias- Variance Tradeoff}
Further, in this section we need to find suitable value for regularization parameter $\lambda$.
Once we are given the conditional distribution $p(t | x)$. A popular loss function in regression problems is squared loss function. Note that 

Squared loss function(decision theory) != sum of squares error function (ML)

The optimal prediction for the squared loss function is given by the conditional expectation h(x)
\begin{align}
    h(x) = E[t | x] = \int t p(t | x)dt
\end{align}
To determine the conditional distribution, we need to use more sophisticated techniques such as regularization. The expected square loss takes the form
\begin{multline}
    E[L] = \int(y(x)-h(x))^2p(x)dx +\\ 
    \int(h(x) - t)^2p(x,t)dx dt \label{eq : expected_loss}
\end{multline}
The second term in Eq \eqref{eq : expected_loss} is due to the noise in the data and represents the minimum achievable value of the expected loss. In the first term, we seek a solution for y(x) so that this term becomes minimum.

With unlimited data and computational resources we could find the regression function h(x) to any accuracy, but practically we only have dataset D and finite number of points, So h(x) is not known exactly.
When we model h(x) using a parametric function y(w,x) with paramter vector $\vec{w}$, From Bayesian point of view, the uncertainity in the model is expressed through a posterior distribution over $\vec{w}$.

A frequentist treatment makes a point estimate of $\vec{w}$ based on the data, the uncertainity of this estimate is expressed through the large dataset consisting finite number of points and each drawn independently from distribution $p(t | x)$.

For any data set D, we learn the algorithm and get a prediction function y(x:D). So, different datasets give different functions which in turn give different squared losses. Averaging over sets gives us the performance of the algorithm.

Consider the integrand of the first term in Eq \eqref{eq : expected_loss}, which becomes
\begin{align}
    (y(x;D) - h(x))^2 \label{eq : first_term}
\end{align}
for a particular data set which has to averaged over the ensemble of sets.
Adding and subtracting $E[y(x;D)]$ to Eq \eqref{eq : first_term}
\begin{align}
    (y(x;D) - E[y(x;D)] + E[y(x;D)]- h(x))^2
\end{align}
Expanding,
\begin{multline}
    \brak{y(x;D) - E[y(x;D)] + E[y(x;D)]- h(x)}^2 = \\
      \brak{y(x;D) - E[y(x;D)]}^2 + \brak{E[y(x;D)]- h(x)}^2 + \\
      2\brak{y(x;D) - E[y(x;D)]}\brak{E[y(x;D)]- h(x)}^2)
\end{multline}
Take the expectation w.r.t D,
\begin{multline}
    E[(y(x;D) - h(x))^2] = \brak{E[y(x;D)]- h(x)}^2\\
       + E_{D}[(y(x;D) - E_{D}[y(x;D)])^2] \label{eq : modified_expected}
\end{multline}
In Eq \eqref{eq : modified_expected}, 

the first term refers to $(bias)^2$ - which represents the extent to which the average prediction over all datasets differ from desired regression and

second term refers to $variance$ - which measures the extent to which the solutions for individual data sets vary around their average and hence measuring the extent to which $y(x;D)$
is sensitive to a particular data set

Here, we have considered single input value x. Now substituting this expanded eq in Eq \eqref{eq : expected_loss},
$expected loss = (bias)^2 + variance + noise$

\begin{align}
    (bias)^2 = \int \brak{E[y(x;D)]- h(x)}^2 p(x) dx\\
    variance = \int E_{D}[(y(x;D) - E_{D}[y(x;D)])^2] p(x) dx\\
    noise = \int(h(x) - t)^2p(x,t)dx dt
\end{align}
The ultimate goal is to minimize the expected loss which we have decomposed into $(bias)^2$, $variance$ and constant noise term.

There is a trade off between bias and variance

A flexible model has low bias and high variance

A rigid model has high bias and low variance

The model with optimal predictive capability is the one with best balance.

Consider this example where we generate 100 datasets, each containing 50 data points independently from the sinusoidal curve $h(x) = sin(x)$ . For each dataset, we fit the model by minimizing the regularized error function to give a prediction function.

It is observed that,

large $\lambda$, low variance but high bias and

small $\lambda$, low bias but high variance.

We have to choose to $\lambda$ value such that the value of $(bias)^2 + variance$ is minimum.

Here are few plots of different polynomial degrees with different $\lambda$ values.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=3.4in]{figs/fig2.png}
\end{center}
\caption{}
\label{fig:2}
\end{figure}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=3.4in]{figs/fig3.png}
\end{center}
\caption{}
\label{fig:3}
\end{figure}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=3.4in]{figs/fig4.png}
\end{center}
\caption{}
\label{fig:4}
\end{figure}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=3.4in]{figs/fig5.png}
\end{center}
\caption{}
\label{fig:5}
\end{figure}

In this example, we can also examine the bias - variance trade off quantitatively.

The average prediction is estimated from
\begin{align}
    \bar y(x) = \frac{1}{L} \sum_{l=1}^{L} y^{(l)}(x)
\end{align}
and the integrated $(bias)^2$ and variance is given by
\begin{align}
    (bias)^2 = \frac{1}{N} \sum_{n=1}^{N} \brak{\bar y(x_{n}) - h(x_{n})}^2\\
    variance = \frac{1}{N} \sum_{n=1}^{N} \frac{1}{L} \sum_{l=1}^{L} \brak{y^{(l)}(x_{n}) - \bar y(x_{n})}
\end{align}
Python code:
\begin{lstlisting}
https://github.com/Hrithikraj2/EE4015_IDP/blob/main/Assignment_5/Assignment_5.ipynb
\end{lstlisting}
\end{document}
